{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Idea\n",
    "To generate the dataset, I used Chatgpt by sending it several prompts to create a complete dataset.\n",
    "\n",
    "It generated a sentence, after which it identified which mountain names were in it. This method is the easiest for Chatgpt itself, and thus the least likely to get it wrong, as opposed to the method where Chatgpt should generate more structured or already tokenized data for this task.\n",
    "\n",
    "Then I created `generated_data.txt` combining all Chatgpt answers."
   ],
   "id": "2ae5f8412cdd235b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Prompts Explanation",
   "id": "d6e38a72db15b0c6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "#### Mountain-Specific Prompt: \n",
    "\n",
    "```\n",
    "Generate 200 different natural sentences that include the names of different mountains,  and then identify mentioned mountains. Present the output in the following format (each sentence with new line, without any numbering):\n",
    "The hike to Everest or Elbrus is challenging but rewarding.\n",
    "Everest, Elbrus\n",
    "The view from the top of Mont Blanc is breathtaking.\n",
    "Mont Blanc\n",
    "```\n",
    "\n",
    "It was designed to explicitly mention mountains, helping the model learn to recognize mountain names in sentences where they are likely entities."
   ],
   "id": "8aa772a371388e9f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "#### Mixed Prompt with Other Locations:\n",
    "\n",
    "```\n",
    "In same way generate 200 sentences which include different proper nouns of other places which are not mountains. Also sentences can contain mountains names and other places names. Present the output in the same format (leave blank line if there no mountains in sentence)\n",
    "```\n",
    "\n",
    "It included both mountain names and non-mountain locations to increase the model's robustness in differentiating between mountains and other places."
   ],
   "id": "7ca68fbb3ea8530e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Non-Location-Proper-Names Prompt:\n",
    "\n",
    "```\n",
    "in same way generate 100 sentences which include different proper nouns excepting places.\n",
    "```\n",
    "\n",
    "Now model could distinguish between locations and other proper names in sentences."
   ],
   "id": "e96740042a81e11d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Non-Mountain-Specific Prompt:\n",
    "\n",
    "```\n",
    "in same way generate another 100 sentences which include mountains names, but are obviously not related to mountains from context of the sentence. (so there no mountains name detected)\n",
    "```\n",
    "\n",
    "This prompt included mountain names out of context, so the model could distinguish when the same name does not refer to an entity related to mountains."
   ],
   "id": "a33b3b08c8e5a358"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Formatting data\n",
    "\n",
    "Transform data in next way (basically just get entity position):\n",
    "```\n",
    "The journey to Denali is a test of endurance and skill.\n",
    "Denali\n",
    "```\n",
    "$\\downarrow$\n",
    "```\n",
    "{\n",
    "text: 'The journey to Denali is a test of endurance and skill.',\n",
    "label: [[15, 6, \"MOUNT\"]]\n",
    "}\n",
    "```"
   ],
   "id": "83d44583575e4a0f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T22:44:16.458539Z",
     "start_time": "2024-11-06T22:44:16.353971Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def get_formatted_data(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"UTF-8\") as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    formatted_data = []\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        sentence = lines[i].strip()\n",
    "        if sentence:\n",
    "            i += 1\n",
    "            mountains = lines[i].strip().split(\", \")\n",
    "            labels = [] \n",
    "            if mountains[0] != \"\":  # No mountains name in sentence\n",
    "                for mountain in mountains:\n",
    "                    match = re.search(re.escape(mountain), sentence)\n",
    "                    if match:\n",
    "                        start_idx = match.start()\n",
    "                        labels.append([start_idx, len(mountain), \"MOUNT\"])\n",
    "    \n",
    "            formatted_data.append({\n",
    "                \"text\": sentence,\n",
    "                \"label\": labels\n",
    "            })\n",
    "        i += 1\n",
    "    \n",
    "    return formatted_data"
   ],
   "id": "5bacc29d23a5ff83",
   "outputs": [],
   "execution_count": 135
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T22:44:29.198876Z",
     "start_time": "2024-11-06T22:44:29.104219Z"
    }
   },
   "cell_type": "code",
   "source": "data = get_formatted_data('data/generated_dataset.txt')",
   "id": "f126283857c6fe68",
   "outputs": [],
   "execution_count": 136
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Split Dataset",
   "id": "2f727c1b0093366"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data = train_test_split(data, test_size=0.15, random_state=42)"
   ],
   "id": "20b8edec2b1ba086"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T22:45:36.099314Z",
     "start_time": "2024-11-06T22:45:36.067854Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "train_counts = Counter()\n",
    "test_counts = Counter()\n",
    "for item in train_data:\n",
    "    train_counts[len(item['label'])] += 1\n",
    "for item in test_data:\n",
    "    test_counts[len(item['label'])] += 1"
   ],
   "id": "5216e03dab5e2e11",
   "outputs": [],
   "execution_count": 137
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T22:46:13.640060Z",
     "start_time": "2024-11-06T22:46:13.631767Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Mountain distribution in train sentences:\", train_counts)\n",
    "print(\"Mountain distribution in test sentences:\", test_counts)"
   ],
   "id": "744de7830c5c0eeb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mountain distribution in train sentences: Counter({0: 250, 1: 164, 2: 5})\n",
      "Mountain distribution in test sentences: Counter({0: 42, 1: 30, 2: 2})\n"
     ]
    }
   ],
   "execution_count": 139
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "It's not the best distribution, but still fine.",
   "id": "20fddc68ec3d5ae6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T23:01:57.651493Z",
     "start_time": "2024-11-06T23:01:57.635341Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "# Save datasets\n",
    "with open('data/train_data.json', 'w') as f:\n",
    "    json.dump(train_data, f)\n",
    "with open('data/test_data.json', 'w') as f:\n",
    "    json.dump(test_data, f)"
   ],
   "id": "59cc8b50b572f749",
   "outputs": [],
   "execution_count": 154
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Tokenization",
   "id": "59740f3fe04be2b3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now in order to use this data for NER training, it should be tokenized and labeled in a format compatible with NER models, such as the CoNLL format. Each sentence would appear tokenized, with each token labeled as either B-MOUNT (beginning of a mountain name), I-MOUNT (inside a mountain name), or 0 (outside of any named entity).",
   "id": "ff866d3997da06d9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T22:50:53.249939Z",
     "start_time": "2024-11-06T22:50:52.253609Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-multilingual-cased\")"
   ],
   "id": "a37be2303174c9bc",
   "outputs": [],
   "execution_count": 140
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T22:53:06.427968Z",
     "start_time": "2024-11-06T22:53:06.369460Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize_data(dataset):\n",
    "    tokenized_data = []\n",
    "    \n",
    "    for sample in dataset:\n",
    "        text = sample[\"text\"]\n",
    "        entities = sample[\"label\"]\n",
    "        tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(text)))\n",
    "        labels = [0] * len(tokens)\n",
    "        \n",
    "        # Label mountain entities\n",
    "        for start, length, _ in entities:\n",
    "            prefix_tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(text[:start])))\n",
    "            start_token = len(prefix_tokens) - 1\n",
    "            \n",
    "            entity_tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(text[start:start+length])))\n",
    "            to_ignore = entity_tokens.count(\"[SEP]\") + entity_tokens.count(\"[CLS]\")\n",
    "            end_token = start_token + len(entity_tokens) - 1 - to_ignore\n",
    "            \n",
    "            labels[start_token] = 1  # B-MOUNT\n",
    "            for idx in range(start_token+1, end_token+1):\n",
    "                labels[idx] = 2  # I-MOUNT\n",
    "\n",
    "        tokens_ids = tokenizer.convert_tokens_to_ids(tokens)   \n",
    "        tokenized_data.append({\n",
    "            'input_ids': tokens_ids,\n",
    "            'labels': labels\n",
    "        })\n",
    "    return tokenized_data"
   ],
   "id": "510b7489f82b59e3",
   "outputs": [],
   "execution_count": 142
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T22:53:18.461314Z",
     "start_time": "2024-11-06T22:53:18.009289Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_train_data = tokenize_data(train_data)\n",
    "tokenized_test_data = tokenize_data(test_data)"
   ],
   "id": "8020d15ab89ed534",
   "outputs": [],
   "execution_count": 143
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T22:54:16.105354Z",
     "start_time": "2024-11-06T22:54:15.856581Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "train_ds = Dataset.from_pandas(pd.DataFrame(data=tokenized_train_data))\n",
    "test_ds = Dataset.from_pandas(pd.DataFrame(data=tokenized_test_data))"
   ],
   "id": "ce12f783c0e17626",
   "outputs": [],
   "execution_count": 144
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Example:",
   "id": "e8ab903804231840"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T22:54:55.503921Z",
     "start_time": "2024-11-06T22:54:55.496330Z"
    }
   },
   "cell_type": "code",
   "source": [
    "example = train_ds[1]\n",
    "input_ids = example[\"input_ids\"]\n",
    "labels = example[\"labels\"]\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "print(\"Ids:\", input_ids)\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Labels:\", labels)"
   ],
   "id": "3aacafd933971bc5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ids: [101, 33745, 10124, 11053, 10142, 10474, 42235, 12898, 12221, 10111, 23704, 20783, 119, 102]\n",
      "Tokens: ['[CLS]', 'Prague', 'is', 'known', 'for', 'its', 'beautiful', 'old', 'town', 'and', 'historic', 'architecture', '.', '[SEP]']\n",
      "Labels: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "execution_count": 146
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T22:57:10.931145Z",
     "start_time": "2024-11-06T22:57:10.646526Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save data\n",
    "train_ds.save_to_disk(\"data/train_dataset\")\n",
    "test_ds.save_to_disk(\"data/test_dataset\")"
   ],
   "id": "6c819242c206a84e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/419 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1a4e58560c624aa5b9950a324d2b1563"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/74 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b7963a054ec648edb6c1e037979b4a27"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 148
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9dfaacba1c14a886"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
